{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "post_training_quant.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/post_training_quant_fashion_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8E0lw5eYWm"
      },
      "source": [
        "# Post-training dynamic range quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIGrZZPTZVeO"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/post_training_quant_fashion_mnist.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/post_training_quant_fashion_mnist.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTC1rDAuei_1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite/)  converts weights to 8 bit floating point as it converts tensorflow [graphdefs](https://haosdent.gitbooks.io/tensorflow-document/content/resources/data_versions.html) to TensorFlow Lite flat buffer format. Dynamic range quantization (also called post training quantizaton) is able to achive 4x reduction in model size. TFLit also supports using quantized kernels for faster implementation and mixing floating point kernels with quantized kernels in the same graph.\n",
        "Activations are stored in floating port, are quantized to 8 bits of precision and then are de-quantized to floating point precision after processing.\n",
        "\n",
        "In this technique the weights are quantized post training. Model weights are not retrained to compensate for quantization so make sure you test the model for accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XsEP17Zelz9"
      },
      "source": [
        "## Build an Fashion MNIST model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqqUIZjZjac"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyqAw1M9lyab"
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pathlib"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ6Q0qqKZogR"
      },
      "source": [
        "### Train a TensorFlow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWSAjQWagIHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db3f2c6-f11b-49e8-9830-690c773e16b2"
      },
      "source": [
        "# Load Fashion MNIST dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=1,\n",
        "  validation_data=(test_images, test_labels)\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5152 - accuracy: 0.8193 - val_loss: 0.4285 - val_accuracy: 0.8513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa48a358910>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NMaNZQCkW9X"
      },
      "source": [
        "For the example, since you trained the model for just a single epoch, so it only trains to ~81% to 83% accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl8_fzVAZwOh"
      },
      "source": [
        "### Convert to a TensorFlow Lite model\n",
        "\n",
        "Using the Python [TFLiteConverter](https://www.tensorflow.org/lite/convert/python_api), you can now convert the trained model into a TensorFlow Lite model.\n",
        "\n",
        "Now load the model using the `TFLiteConverter`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i8B2nDZmAgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41275b96-fb77-4be5-89f0-1788df8c2550"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmptdmqwtpq/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2o2ZfF0aiCx"
      },
      "source": [
        "Write it out to a tflite file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vptWZq2xnclo"
      },
      "source": [
        "tflite_models_dir = pathlib.Path(\"/tmp/fashion_mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie9pQaQrn5ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c257749b-7531-4c20-aa14-ec0acbfa58ef"
      },
      "source": [
        "tflite_model_file = tflite_models_dir/\"fashion_mnist_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84500"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BONhYtYocQY"
      },
      "source": [
        "To quantize the model on export, set the `optimizations` flag to optimize for size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8PUvLWDlmmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864cf2ae-8e1f-424a-f6e4-83dc41147295"
      },
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"fashion_mnist_model.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_quant_model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpzqj403v1/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpzqj403v1/assets\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23904"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhMmUTl4sbkz"
      },
      "source": [
        "Note how the resulting file, is approximately `1/4` the size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JExfcfLDscu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3201374c-b294-4f0a-b2fc-d118dc6a105f"
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24K\n",
            "-rw-r--r-- 1 root root 24K Sep 19 04:10 fashion_mnist_model.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8lQHMp_asCq"
      },
      "source": [
        "## Run the TFLite models\n",
        "\n",
        "Run the TensorFlow Lite model using the Python TensorFlow Lite\n",
        "Interpreter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap_jE7QRvhPf"
      },
      "source": [
        "### Load the model into an interpreter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn16Rc23zTss"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Pztk1mvNVL"
      },
      "source": [
        "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "interpreter_quant.allocate_tensors()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2opUt_JTdyEu"
      },
      "source": [
        "### Test the model on one image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKslvo2kwWac"
      },
      "source": [
        "test_image = np.expand_dims(test_images[1], axis=0).astype(np.float32)\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "interpreter.set_tensor(input_index, test_image)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor(output_index)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZClM2vo3_bm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "dc3ef1af-a5cb-4cef-d186-99e50b9c3523"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "plt.imshow(test_images[1])\n",
        "template = \"True:{true}, predicted:{predict}\"\n",
        "_ = plt.title(template.format(true= str(test_labels[1]),\n",
        "                              predict=str(np.argmax(predictions[0]))))\n",
        "plt.grid(False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwN7uIdCd8Gw"
      },
      "source": [
        "### Evaluate the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05aeAuWjvjPx"
      },
      "source": [
        "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for test_image in test_images:\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  accurate_count = 0\n",
        "  for index in range(len(prediction_digits)):\n",
        "    if prediction_digits[index] == test_labels[index]:\n",
        "      accurate_count += 1\n",
        "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqXBnDfJ7qxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56ccd2a-2388-4232-f937-58339c57a195"
      },
      "source": [
        "print(evaluate_model(interpreter))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCaKDPpZZ1fa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km3cY9ry8ZlG"
      },
      "source": [
        "Repeat the evaluation on the dynamic range quantized model to obtain:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9cnwiPp6EGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d33ad9-b74b-4c7f-bd8c-3bbd65f25bc0"
      },
      "source": [
        "print(evaluate_model(interpreter_quant))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lfxkor8pgv"
      },
      "source": [
        "In this example, the compressed model has no difference at all in the accuracy."
      ]
    }
  ]
}