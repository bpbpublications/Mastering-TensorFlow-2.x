{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/marload/DeepRL-TensorFlow2/blob/master/DQN/DQN_Discrete.py\n",
    "https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "#wandb.init(name='DQN', project=\"deep-rl-tf2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self):\n",
    "        sample = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n",
    "        states = np.array(states).reshape(self.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(self.batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionStateModel:\n",
    "    def __init__(self, state_dim, aciton_dim):\n",
    "        self.state_dim  = state_dim\n",
    "        self.action_dim = aciton_dim\n",
    "        self.epsilon = 1.0 #args.eps\n",
    "        self.lr = 0.005\n",
    "        self.eps_min = 0.01\n",
    "        self.eps_decay = 0.995\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(self.action_dim)\n",
    "        ])\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(self.lr))\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        self.epsilon *= self.eps_decay\n",
    "        self.epsilon = max(self.epsilon, self.eps_min)\n",
    "        q_value = self.predict(state)[0]\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim-1)\n",
    "        return np.argmax(q_value)\n",
    "\n",
    "    def train(self, states, targets):\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "\n",
    "        self.model = ActionStateModel(self.state_dim, self.action_dim)\n",
    "        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\n",
    "        self.target_update()\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "    def target_update(self):\n",
    "        weights = self.model.model.get_weights()\n",
    "        self.target_model.model.set_weights(weights)\n",
    "    \n",
    "    def replay(self):\n",
    "        for _ in range(10):\n",
    "            states, actions, rewards, next_states, done = self.buffer.sample()\n",
    "            targets = self.target_model.predict(states)\n",
    "            next_q_values = self.target_model.predict(next_states).max(axis=1)\n",
    "            targets[range(self.batch_size), actions] = rewards + (1-done) * next_q_values * self.gamma\n",
    "            self.model.train(states, targets)\n",
    "    \n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in range(max_episodes):\n",
    "            done, total_reward = False, 0\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.model.get_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.put(state, action, reward*0.01, next_state, done)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            if self.buffer.size() >= self.batch_size:\n",
    "                self.replay()\n",
    "            self.target_update()\n",
    "            print('EP{} EpisodeReward={}'.format(ep, total_reward))\n",
    "            wandb.log({'Reward': total_reward})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gamma', type=float, default=0.95)\n",
    "parser.add_argument('--lr', type=float, default=0.005)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--eps', type=float, default=1.0)\n",
    "parser.add_argument('--eps_decay', type=float, default=0.995)\n",
    "parser.add_argument('--eps_min', type=float, default=0.01)\n",
    "\n",
    "args = parser.parse_args()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0 EpisodeReward=21.0\n",
      "EP1 EpisodeReward=26.0\n",
      "EP2 EpisodeReward=13.0\n",
      "EP3 EpisodeReward=23.0\n",
      "EP4 EpisodeReward=14.0\n",
      "EP5 EpisodeReward=10.0\n",
      "EP6 EpisodeReward=19.0\n",
      "EP7 EpisodeReward=12.0\n",
      "EP8 EpisodeReward=18.0\n",
      "EP9 EpisodeReward=17.0\n",
      "EP10 EpisodeReward=23.0\n",
      "EP11 EpisodeReward=18.0\n",
      "EP12 EpisodeReward=19.0\n",
      "EP13 EpisodeReward=14.0\n",
      "EP14 EpisodeReward=20.0\n",
      "EP15 EpisodeReward=22.0\n",
      "EP16 EpisodeReward=13.0\n",
      "EP17 EpisodeReward=10.0\n",
      "EP18 EpisodeReward=46.0\n",
      "EP19 EpisodeReward=98.0\n",
      "EP20 EpisodeReward=92.0\n",
      "EP21 EpisodeReward=150.0\n",
      "EP22 EpisodeReward=100.0\n",
      "EP23 EpisodeReward=500.0\n",
      "EP24 EpisodeReward=389.0\n",
      "EP25 EpisodeReward=142.0\n",
      "EP26 EpisodeReward=140.0\n",
      "EP27 EpisodeReward=299.0\n",
      "EP28 EpisodeReward=358.0\n",
      "EP29 EpisodeReward=215.0\n",
      "EP30 EpisodeReward=170.0\n",
      "EP31 EpisodeReward=133.0\n",
      "EP32 EpisodeReward=105.0\n",
      "EP33 EpisodeReward=134.0\n",
      "EP34 EpisodeReward=142.0\n",
      "EP35 EpisodeReward=217.0\n",
      "EP36 EpisodeReward=117.0\n",
      "EP37 EpisodeReward=168.0\n",
      "EP38 EpisodeReward=84.0\n",
      "EP39 EpisodeReward=91.0\n",
      "EP40 EpisodeReward=98.0\n",
      "EP41 EpisodeReward=78.0\n",
      "EP42 EpisodeReward=154.0\n",
      "EP43 EpisodeReward=127.0\n",
      "EP44 EpisodeReward=358.0\n",
      "EP45 EpisodeReward=95.0\n",
      "EP46 EpisodeReward=155.0\n",
      "EP47 EpisodeReward=141.0\n",
      "EP48 EpisodeReward=112.0\n",
      "EP49 EpisodeReward=92.0\n",
      "EP50 EpisodeReward=116.0\n",
      "EP51 EpisodeReward=94.0\n",
      "EP52 EpisodeReward=132.0\n",
      "EP53 EpisodeReward=345.0\n",
      "EP54 EpisodeReward=153.0\n",
      "EP55 EpisodeReward=154.0\n",
      "EP56 EpisodeReward=349.0\n",
      "EP57 EpisodeReward=270.0\n",
      "EP58 EpisodeReward=188.0\n",
      "EP59 EpisodeReward=137.0\n",
      "EP60 EpisodeReward=148.0\n",
      "EP61 EpisodeReward=212.0\n",
      "EP62 EpisodeReward=141.0\n",
      "EP63 EpisodeReward=130.0\n",
      "EP64 EpisodeReward=187.0\n",
      "EP65 EpisodeReward=180.0\n",
      "EP66 EpisodeReward=123.0\n",
      "EP67 EpisodeReward=150.0\n",
      "EP68 EpisodeReward=135.0\n",
      "EP69 EpisodeReward=164.0\n",
      "EP70 EpisodeReward=143.0\n",
      "EP71 EpisodeReward=171.0\n",
      "EP72 EpisodeReward=161.0\n",
      "EP73 EpisodeReward=261.0\n",
      "EP74 EpisodeReward=140.0\n",
      "EP75 EpisodeReward=238.0\n",
      "EP76 EpisodeReward=177.0\n",
      "EP77 EpisodeReward=196.0\n",
      "EP78 EpisodeReward=152.0\n",
      "EP79 EpisodeReward=209.0\n",
      "EP80 EpisodeReward=238.0\n",
      "EP81 EpisodeReward=160.0\n",
      "EP82 EpisodeReward=135.0\n",
      "EP83 EpisodeReward=170.0\n",
      "EP84 EpisodeReward=168.0\n",
      "EP85 EpisodeReward=306.0\n",
      "EP86 EpisodeReward=196.0\n",
      "EP87 EpisodeReward=179.0\n",
      "EP88 EpisodeReward=182.0\n",
      "EP89 EpisodeReward=446.0\n",
      "EP90 EpisodeReward=121.0\n",
      "EP91 EpisodeReward=245.0\n",
      "EP92 EpisodeReward=161.0\n",
      "EP93 EpisodeReward=228.0\n",
      "EP94 EpisodeReward=153.0\n",
      "EP95 EpisodeReward=215.0\n",
      "EP96 EpisodeReward=199.0\n",
      "EP97 EpisodeReward=137.0\n",
      "EP98 EpisodeReward=200.0\n",
      "EP99 EpisodeReward=187.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent(env)\n",
    "agent.train(max_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python37764bittfconda003636c015c146309c74c3bc0052f30e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
