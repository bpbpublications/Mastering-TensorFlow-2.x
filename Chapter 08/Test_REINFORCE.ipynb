{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_REINFORCE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIwAjg4mEVxIF0daibmNtf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/Test_REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7R8WM6iGQCs",
        "outputId": "1f2de7c2-bd98-4715-e49a-f29cd13f60cd"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "#!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.19.6-1ubuntu4.8_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Collecting imageio==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/64/8e2bb6aac43d6ed7c2d9514320b43d5e80c00f150ee2b9408aee24359e6d/imageio-2.4.0.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-cp37-none-any.whl size=3303881 sha256=360a7f1dcd4de1eb3a34f820414774ec0d4e1185c33f58fe555e317b26f267f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/83/88/a1cba54ac06395d9e4ddcd9cf06911cd0b26cd78af9a61071b\n",
            "Successfully built imageio\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "Successfully installed imageio-2.4.0\n",
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/cd/a0710b1caae042b7a4d54fc74073fb4df7adf073934798443bdc0059813a/tf_agents-0.7.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.3->tf-agents) (54.2.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k2WE0MzGVkk"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl.testing import parameterized\n",
        "from absl.testing.absltest import mock\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tf_agents.agents.reinforce import reinforce_agent\n",
        "from tf_agents.networks import actor_distribution_rnn_network\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.networks import utils as network_utils\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import policy_step\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.utils import nest_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhRcxmL-HQCo"
      },
      "source": [
        "class DummyActorNet(network.Network):\n",
        "\n",
        "  def __init__(self,\n",
        "               input_tensor_spec,\n",
        "               output_tensor_spec,\n",
        "               unbounded_actions=False,\n",
        "               stateful=False):\n",
        "    # When unbounded_actions=True, we skip the final tanh activation and the\n",
        "    # action shift and scale. This allows us to compute the actor and critic\n",
        "    # losses by hand more easily.\n",
        "    # If stateful=True, the network state has the same shape as\n",
        "    # `input_tensor_spec`. Otherwise it is empty.\n",
        "    state_spec = (tf.TensorSpec(input_tensor_spec.shape, tf.float32)\n",
        "                  if stateful else ())\n",
        "    super(DummyActorNet, self).__init__(\n",
        "        input_tensor_spec=input_tensor_spec,\n",
        "        state_spec=state_spec,\n",
        "        name='DummyActorNet')\n",
        "    single_action_spec = tf.nest.flatten(output_tensor_spec)[0]\n",
        "    activation_fn = None if unbounded_actions else tf.nn.tanh\n",
        "    self._output_tensor_spec = output_tensor_spec\n",
        "    self._dummy_layers = [\n",
        "        tf.keras.layers.Dense(\n",
        "            single_action_spec.shape.num_elements() * 2,\n",
        "            activation=activation_fn,\n",
        "            kernel_initializer=tf.constant_initializer([[2, 1], [1, 1]]),\n",
        "            bias_initializer=tf.constant_initializer(5),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "  def call(self, observations, step_type, network_state):\n",
        "    del step_type\n",
        "\n",
        "    states = tf.cast(tf.nest.flatten(observations)[0], tf.float32)\n",
        "    for layer in self._dummy_layers:\n",
        "      states = layer(states)\n",
        "\n",
        "    single_action_spec = tf.nest.flatten(self._output_tensor_spec)[0]\n",
        "    # action_spec is TensorSpec([1], ...) so make sure there's an outer dim.\n",
        "    actions = states[..., 0]\n",
        "    stdevs = states[..., 1]\n",
        "    actions = tf.reshape(actions, [-1] + single_action_spec.shape.as_list())\n",
        "    stdevs = tf.reshape(stdevs, [-1] + single_action_spec.shape.as_list())\n",
        "    actions = tf.nest.pack_sequence_as(self._output_tensor_spec, [actions])\n",
        "    stdevs = tf.nest.pack_sequence_as(self._output_tensor_spec, [stdevs])\n",
        "\n",
        "    distribution = nest_utils.map_structure_up_to(\n",
        "        self._output_tensor_spec,\n",
        "        tfp.distributions.MultivariateNormalDiag,\n",
        "        actions,\n",
        "        stdevs)\n",
        "    return distribution, network_state"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQzZPWRm3kQL"
      },
      "source": [
        "class ReinforceAgentBase(tf.test.TestCase, parameterized.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    super(ReinforceAgentBase, self).setUp()\n",
        "    tf.compat.v1.enable_resource_variables()\n",
        "    self._obs_spec = tensor_spec.TensorSpec([2], tf.float32)\n",
        "    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n",
        "    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NQv7kEG1Vb5"
      },
      "source": [
        "### Policy Gradient Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZFvLgNM1Tpm"
      },
      "source": [
        "class ReinforceAgentSampleOne(ReinforceAgentBase):\n",
        "\n",
        "  def setUp(self):\n",
        "    super(ReinforceAgentSampleOne, self).setUp()\n",
        "  \n",
        "  def runPolicyGradientLoss(self):\n",
        "    agent = reinforce_agent.ReinforceAgent(\n",
        "        self._time_step_spec,\n",
        "        self._action_spec,\n",
        "        actor_network=DummyActorNet(\n",
        "            self._obs_spec, self._action_spec, unbounded_actions=True),\n",
        "        optimizer=None,\n",
        "    )\n",
        "    observations = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "    time_steps = ts.restart(observations, batch_size=2)\n",
        "    actions = tf.constant([[0], [1]], dtype=tf.float32)\n",
        "    actions_distribution = agent.collect_policy.distribution(\n",
        "        time_steps).action\n",
        "    returns = tf.constant([1.9, 1.0], dtype=tf.float32)\n",
        " \n",
        "    loss = agent.policy_gradient_loss(\n",
        "        actions_distribution, actions, time_steps.is_last(), returns, 1)\n",
        " \n",
        "    self.evaluate(tf.compat.v1.global_variables_initializer())\n",
        "    loss_ = self.evaluate(loss)\n",
        "    tf.print(\"loss:\", loss, output_stream=sys.stdout)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY3Kte_XHf0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8bede1b-cb52-4963-deb3-abd3d84e5f80"
      },
      "source": [
        "sampleOne = ReinforceAgentSampleOne()\n",
        "sampleOne.setUp()\n",
        "sampleOne.runPolicyGradientLoss()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 10.9836674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ori06gX0C3AG"
      },
      "source": [
        "### Multiple Episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJxdpPMm5fVA"
      },
      "source": [
        "class ReinforceAgentSampleTwo(ReinforceAgentBase):\n",
        "\n",
        "  def setUp(self):\n",
        "    super(ReinforceAgentSampleTwo, self).setUp()\n",
        "  \n",
        "  def runPolicyGradientLossMultipleEpisodes(self):\n",
        "    agent = reinforce_agent.ReinforceAgent(\n",
        "        self._time_step_spec,\n",
        "        self._action_spec,\n",
        "        actor_network=DummyActorNet(\n",
        "            self._obs_spec, self._action_spec, unbounded_actions=True),\n",
        "        optimizer=None,\n",
        "    )\n",
        "\n",
        "    step_type = tf.constant(\n",
        "        [ts.StepType.FIRST, ts.StepType.LAST, ts.StepType.FIRST,\n",
        "         ts.StepType.LAST])\n",
        "    reward = tf.constant([0, 0, 0, 0], dtype=tf.float32)\n",
        "    discount = tf.constant([1, 1, 1, 1], dtype=tf.float32)\n",
        "    observations = tf.constant(\n",
        "        [[1, 2], [1, 2], [1, 2], [1, 2]], dtype=tf.float32)\n",
        "    time_steps = ts.TimeStep(step_type, reward, discount, observations)\n",
        "\n",
        "    actions = tf.constant([[0], [1], [2], [3]], dtype=tf.float32)\n",
        "    actions_distribution = agent.collect_policy.distribution(\n",
        "        time_steps).action\n",
        "    returns = tf.constant([1.9, 1.9, 1.0, 1.0], dtype=tf.float32)\n",
        "\n",
        "    loss = agent.policy_gradient_loss(\n",
        "        actions_distribution, actions, time_steps.is_last(), returns, 2)\n",
        "\n",
        "    self.evaluate(tf.compat.v1.global_variables_initializer())\n",
        "    loss_ = self.evaluate(loss)\n",
        "    tf.print(\"loss:\", loss, output_stream=sys.stdout)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y8ujZWR508_",
        "outputId": "f500e134-ef70-4ed7-8b0c-af4f6659be58"
      },
      "source": [
        "sampleTwo = ReinforceAgentSampleTwo()\n",
        "sampleTwo.setUp()\n",
        "sampleTwo.runPolicyGradientLossMultipleEpisodes()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 5.14022923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0KczfFcIn31"
      },
      "source": [
        "Bandit Episodes\n",
        "\n",
        "Sample which shows how train reacts correctly to experience when there is only a single Bandit episode.  Bandit episodes are encoded differently than MDP episodes.  They have only a single transition with `step_type=StepType.FIRST` and `next_step_type=StepType.LAST`.\n",
        "\n",
        "```\n",
        "F, L, M = ts.StepType.{FIRST, MID, LAST} in the chart below.\n",
        "\n",
        "Experience looks like this:\n",
        "Trajectories: (F, L)\n",
        "observation : [1, 2]\n",
        "action      :   [0]\n",
        "reward      :    3\n",
        "~is_boundary:    0\n",
        "is_last     :    1\n",
        "valid reward:   3*1\n",
        "```\n",
        "The single bandit transition is valid and not masked.\n",
        "\n",
        "The expected_loss is `> 0.0` in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBJtxZGKImiG",
        "outputId": "cf0be812-01a0-45e2-8eb0-293b7ee958bb"
      },
      "source": [
        "agent._optimizer = tf.compat.v1.train.AdamOptimizer(0.001)\n",
        "\n",
        "step_type = tf.constant([ts.StepType.FIRST])\n",
        "next_step_type = tf.constant([ts.StepType.LAST])\n",
        "reward = tf.constant([3], dtype=tf.float32)\n",
        "discount = tf.constant([0], dtype=tf.float32)\n",
        "observations = tf.constant([[1, 2]], dtype=tf.float32)\n",
        "actions = tf.constant([[0]], dtype=tf.float32)\n",
        "\n",
        "experience = nest_utils.batch_nested_tensors(trajectory.Trajectory(\n",
        "        step_type, observations, actions, (), next_step_type, reward, discount))\n",
        "\n",
        "# Rewards should be counted.\n",
        "expected_loss = 10.8935775757\n",
        "\n",
        "if tf.executing_eagerly():\n",
        "      loss = agent.train(experience)\n",
        "\n",
        "\n",
        "testc =  tf.test.TestCase()\n",
        "testc.evaluate(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "\n",
        "testc.evaluate(loss)\n",
        "\n",
        "tf.print(\"loss:\", loss, output_stream=sys.stdout)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: LossInfo(loss=0, extra=ReinforceAgentLossInfo(policy_gradient_loss=\"policy_gradient_loss\", policy_network_regularization_loss=\"policy_network_regularization_loss\", entropy_regularization_loss=\"entropy_regularization_loss\", value_estimation_loss=\"value_estimation_loss\", value_network_regularization_loss=\"value_network_regularization_loss\"))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}