{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sample_One_SAC_Agent.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMKFE6B8kjAY2u/da8HJX3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/Sample_One_SAC_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7R8WM6iGQCs",
        "outputId": "00b0a1d5-addb-4456-e5ad-aa0279161c3a"
      },
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'imageio==2.4.0'\n",
        "#!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8\n",
            "  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.19.6-1ubuntu4.8_amd64.deb  404  Not Found [IP: 91.189.88.152 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Collecting imageio==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/64/8e2bb6aac43d6ed7c2d9514320b43d5e80c00f150ee2b9408aee24359e6d/imageio-2.4.0.tar.gz (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.0) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.0-cp37-none-any.whl size=3303881 sha256=3184c5072e1e413246a9cdfb09571bd3c53955a2ffe512d214022b836433c3be\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/83/88/a1cba54ac06395d9e4ddcd9cf06911cd0b26cd78af9a61071b\n",
            "Successfully built imageio\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "Successfully installed imageio-2.4.0\n",
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/cd/a0710b1caae042b7a4d54fc74073fb4df7adf073934798443bdc0059813a/tf_agents-0.7.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.1)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.3->tf-agents) (54.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k2WE0MzGVkk"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl.testing import parameterized\n",
        "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
        "\n",
        "import tf_agents\n",
        "import sys\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import network\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.networks import test_utils as networks_test_utils\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import policy_step\n",
        "from tf_agents.trajectories import test_utils as trajectories_test_utils\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.utils import test_utils\n",
        "from tf_agents.agents.sac.sac_agent import SacAgent"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhRcxmL-HQCo"
      },
      "source": [
        "class MyActorPolicy(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               time_step_spec,\n",
        "               action_spec,\n",
        "               actor_network,\n",
        "               training=False):\n",
        "    del time_step_spec\n",
        "    del actor_network\n",
        "    del training\n",
        "    single_action_spec = tf.nest.flatten(action_spec)[0]\n",
        "    # Action is maximum of action range.\n",
        "    self._action = single_action_spec.maximum\n",
        "    self._action_spec = action_spec\n",
        "    self.info_spec = ()\n",
        "\n",
        "  def action(self, time_step):\n",
        "    observation = time_step.observation\n",
        "    batch_size = observation.shape[0]\n",
        "    action = tf.constant(self._action, dtype=tf.float32, shape=[batch_size, 1])\n",
        "    return policy_step.PolicyStep(action=action)\n",
        "\n",
        "  def distribution(self, time_step, policy_state=()):\n",
        "    del policy_state\n",
        "    action = self.action(time_step).action\n",
        "    return policy_step.PolicyStep(action=_MockDistribution(action))\n",
        "\n",
        "  def get_initial_state(self, batch_size):\n",
        "    del batch_size\n",
        "    return ()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQzZPWRm3kQL"
      },
      "source": [
        "class MyCriticNet(network.Network):\n",
        "\n",
        "  def __init__(self, l2_regularization_weight=0.0, shared_layer=None):\n",
        "    super(MyCriticNet, self).__init__(\n",
        "        input_tensor_spec=(tensor_spec.TensorSpec([2], tf.float32),\n",
        "                           tensor_spec.TensorSpec([1], tf.float32)),\n",
        "        state_spec=(),\n",
        "        name=None)\n",
        "    self._l2_regularization_weight = l2_regularization_weight\n",
        "    self._value_layer = tf.keras.layers.Dense(\n",
        "        1,\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2_regularization_weight),\n",
        "        kernel_initializer=tf.constant_initializer([[0], [1]]),\n",
        "        bias_initializer=tf.constant_initializer([[0]]))\n",
        "    self._shared_layer = shared_layer\n",
        "    self._action_layer = tf.keras.layers.Dense(\n",
        "        1,\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(l2_regularization_weight),\n",
        "        kernel_initializer=tf.constant_initializer([[1]]),\n",
        "        bias_initializer=tf.constant_initializer([[0]]))\n",
        "\n",
        "  def copy(self, name=''):\n",
        "    del name\n",
        "    return MyCriticNet(\n",
        "        l2_regularization_weight=self._l2_regularization_weight,\n",
        "        shared_layer=self._shared_layer)\n",
        "\n",
        "  def call(self, inputs, step_type, network_state=()):\n",
        "    del step_type\n",
        "    observation, actions = inputs\n",
        "    actions = tf.cast(tf.nest.flatten(actions)[0], tf.float32)\n",
        "\n",
        "    states = tf.cast(tf.nest.flatten(observation)[0], tf.float32)\n",
        "\n",
        "    s_value = self._value_layer(states)\n",
        "    if self._shared_layer:\n",
        "      s_value = self._shared_layer(s_value)\n",
        "    a_value = self._action_layer(actions)\n",
        "    # Biggest state is best state.\n",
        "    q_value = tf.reshape(s_value + a_value, [-1])\n",
        "    return q_value, network_state"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZFvLgNM1Tpm"
      },
      "source": [
        "def create_sequential_critic_net(l2_regularization_weight=0.0,\n",
        "                                 shared_layer=None):\n",
        "  value_layer = tf.keras.layers.Dense(\n",
        "      1,\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l2_regularization_weight),\n",
        "      kernel_initializer=tf.initializers.constant([[0], [1]]),\n",
        "      bias_initializer=tf.initializers.constant([[0]]))\n",
        "  if shared_layer:\n",
        "    value_layer = sequential.Sequential([value_layer, shared_layer])\n",
        "\n",
        "  action_layer = tf.keras.layers.Dense(\n",
        "      1,\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(l2_regularization_weight),\n",
        "      kernel_initializer=tf.initializers.constant([[1]]),\n",
        "      bias_initializer=tf.initializers.constant([[0]]))\n",
        "\n",
        "  def sum_value_and_action_out(value_and_action_out):\n",
        "    value_out, action_out = value_and_action_out\n",
        "    return tf.reshape(value_out + action_out, [-1])\n",
        "\n",
        "  return sequential.Sequential([\n",
        "      nest_map.NestMap((value_layer, action_layer)),\n",
        "      tf.keras.layers.Lambda(sum_value_and_action_out)\n",
        "  ])\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY3Kte_XHf0i"
      },
      "source": [
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "\n",
        "\n",
        "class SacAgentSample( test_utils.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    super(SacAgentSample, self).setUp()\n",
        "    self._obs_spec = tensor_spec.BoundedTensorSpec([2],\n",
        "                                                   tf.float32,\n",
        "                                                   minimum=0,\n",
        "                                                   maximum=1)\n",
        "    self._time_step_spec = ts.time_step_spec(self._obs_spec)\n",
        "    self._action_spec = tensor_spec.BoundedTensorSpec([1], tf.float32, -1, 1)\n",
        "\n",
        "  #@parameterized.named_parameters(('Network', DummyCriticNet, False),\n",
        "  #                                ('Keras', create_sequential_critic_net, True))\n",
        "  def runCreateAgent(self, create_critic_net_fn, skip_in_tf1):\n",
        "    if skip_in_tf1 and not common.has_eager_been_enabled():\n",
        "      self.skipTest('Skipping test: sequential networks not supported in TF1')\n",
        "\n",
        "    critic_network = create_critic_net_fn()\n",
        "\n",
        "    SacAgent(\n",
        "        self._time_step_spec,\n",
        "        self._action_spec,\n",
        "        critic_network=critic_network,\n",
        "        actor_network=None,\n",
        "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n",
        "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n",
        "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(0.001))\n",
        "        actor_policy_ctor=MyActorPolicy)\n",
        "\n",
        "\n",
        "  def runAgentTrajectoryTrain(self):\n",
        "    actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "        self._obs_spec,\n",
        "        self._action_spec,\n",
        "        fc_layer_params=(10,),\n",
        "        continuous_projection_net=tanh_normal_projection_network\n",
        "        .TanhNormalProjectionNetwork)\n",
        "\n",
        "    agent = SacAgent(\n",
        "        self._time_step_spec,\n",
        "        self._action_spec,\n",
        "        critic_network=MyCriticNet(),\n",
        "        actor_network=actor_net,\n",
        "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n",
        "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(0.001),\n",
        "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(0.001))\n",
        "\n",
        "    trajectory_spec = trajectory.Trajectory(\n",
        "        step_type=self._time_step_spec.step_type,\n",
        "        observation=self._time_step_spec.observation,\n",
        "        action=self._action_spec,\n",
        "        policy_info=(),\n",
        "        next_step_type=self._time_step_spec.step_type,\n",
        "        reward=tensor_spec.BoundedTensorSpec(\n",
        "            [], tf.float32, minimum=0.0, maximum=1.0, name='reward'),\n",
        "        discount=self._time_step_spec.discount)\n",
        "\n",
        "    sample_trajectory_experience = tensor_spec.sample_spec_nest(\n",
        "        trajectory_spec, outer_dims=(3, 2))\n",
        "    loss_info = agent.train(sample_trajectory_experience)\n",
        "    tf.print(loss_info)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a446pqGiLgv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a0ac62b-8d4b-4357-bea2-24c6c8c99f14"
      },
      "source": [
        "sac_agent_1 = SacAgentSample()\n",
        "sac_agent_1.setUp()\n",
        "#'Network', MyCriticNet, False\n",
        "sac_agent_1.runCreateAgent(MyCriticNet,False)\n",
        "sac_agent_1.runAgentTrajectoryTrain()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LossInfo(loss=3.35792017, extra=SacLossInfo(critic_loss=2.24238253, actor_loss=1.11553776, alpha_loss=0))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}