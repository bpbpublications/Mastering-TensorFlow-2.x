{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/1_dqn_tutorial_no_variance_scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmDI-h7cI0tI"
      },
      "source": [
        "# Train a Deep Q Network with TF-Agents\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github.com/rajdeepd/tensorflow_2.0_book_code/blob/master/ch09/1_dqn_tutorial_rd.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "This example shows how to train a [DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  agent on the Cartpole environment using the TF-Agents library.\n",
        "\n",
        "![Cartpole environment](https://raw.githubusercontent.com/tensorflow/agents/master/docs/tutorials/images/cartpole.png)\n",
        "\n",
        "It will walk you through all the components in a Reinforcement Learning (RL) pipeline for training, evaluation and data collection.\n",
        "\n",
        "\n",
        "To run this code live, click the 'Run in Google Colab' link above. \n",
        "\n",
        "It references the original sample available at https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "Get right version of imgaug library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7DG6vAJU03p"
      },
      "source": [
        "!pip show imgaug\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWdJ_7yCc-fR"
      },
      "source": [
        "#!pip uninstall imgaug\n",
        "#!pip install imgaug==0.2.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NWRXoWIdP2y"
      },
      "source": [
        "If you haven't installed the following dependencies, run:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goom8dCcdNvm"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:08:59.027365Z",
          "iopub.status.busy": "2021-02-09T12:08:59.026669Z",
          "iopub.status.idle": "2021-02-09T12:09:41.076422Z",
          "shell.execute_reply": "2021-02-09T12:09:41.076909Z"
        },
        "id": "KEHR2Ui-lo8O"
      },
      "source": [
        "\n",
        "#!sudo apt-get install -y xvfb ffmpeg\n",
        "#!pip install -q 'imageio==2.4.0'\n",
        "#!pip install -q pyvirtualdisplay\n",
        "#!pip install -q tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:41.083676Z",
          "iopub.status.busy": "2021-02-09T12:09:41.082944Z",
          "iopub.status.idle": "2021-02-09T12:09:49.550074Z",
          "shell.execute_reply": "2021-02-09T12:09:49.549340Z"
        },
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:49.554798Z",
          "iopub.status.busy": "2021-02-09T12:09:49.554045Z",
          "iopub.status.idle": "2021-02-09T12:09:50.324476Z",
          "shell.execute_reply": "2021-02-09T12:09:50.323801Z"
        },
        "id": "J6HsdS5GbSjd"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.331133Z",
          "iopub.status.busy": "2021-02-09T12:09:50.330425Z",
          "iopub.status.idle": "2021-02-09T12:09:50.333928Z",
          "shell.execute_reply": "2021-02-09T12:09:50.334396Z"
        },
        "id": "NspmzG4nP3b9"
      },
      "source": [
        "tf.version.VERSION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.339058Z",
          "iopub.status.busy": "2021-02-09T12:09:50.338424Z",
          "iopub.status.idle": "2021-02-09T12:09:50.341044Z",
          "shell.execute_reply": "2021-02-09T12:09:50.340536Z"
        },
        "id": "HC1kNrOsLSIZ"
      },
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
        "\n",
        "Load the CartPole environment from the OpenAI Gym suite. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.345154Z",
          "iopub.status.busy": "2021-02-09T12:09:50.344499Z",
          "iopub.status.idle": "2021-02-09T12:09:50.354562Z",
          "shell.execute_reply": "2021-02-09T12:09:50.354020Z"
        },
        "id": "pYEz-S9gEv2-"
      },
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.360958Z",
          "iopub.status.busy": "2021-02-09T12:09:50.360255Z",
          "iopub.status.idle": "2021-02-09T12:09:50.939520Z",
          "shell.execute_reply": "2021-02-09T12:09:50.938861Z"
        },
        "id": "RlO7WIQHu_7D"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
        "\n",
        "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.949029Z",
          "iopub.status.busy": "2021-02-09T12:09:50.948343Z",
          "iopub.status.idle": "2021-02-09T12:09:50.950744Z",
          "shell.execute_reply": "2021-02-09T12:09:50.951201Z"
        },
        "id": "exDv57iHfwQV"
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.955678Z",
          "iopub.status.busy": "2021-02-09T12:09:50.954940Z",
          "iopub.status.idle": "2021-02-09T12:09:50.957214Z",
          "shell.execute_reply": "2021-02-09T12:09:50.957661Z"
        },
        "id": "UxiSyCbBUQPi"
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E9cVnObntBe"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.961959Z",
          "iopub.status.busy": "2021-02-09T12:09:50.961281Z",
          "iopub.status.idle": "2021-02-09T12:09:50.964201Z",
          "shell.execute_reply": "2021-02-09T12:09:50.963664Z"
        },
        "id": "bttJ4uxZUQBr"
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "In the Cartpole environment:\n",
        "\n",
        "-   `observation` is an array of 4 floats: \n",
        "    -   the position and velocity of the cart\n",
        "    -   the angular position and velocity of the pole \n",
        "-   `reward` is a scalar float value\n",
        "-   `action` is a scalar integer with only two possible values:\n",
        "    -   `0` — \"move left\"\n",
        "    -   `1` — \"move right\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.972661Z",
          "iopub.status.busy": "2021-02-09T12:09:50.971968Z",
          "iopub.status.idle": "2021-02-09T12:09:50.974680Z",
          "shell.execute_reply": "2021-02-09T12:09:50.975090Z"
        },
        "id": "V2UGR5t_iZX-"
      },
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Usually two environments are instantiated: one for training and one for evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.981525Z",
          "iopub.status.busy": "2021-02-09T12:09:50.980855Z",
          "iopub.status.idle": "2021-02-09T12:09:50.982667Z",
          "shell.execute_reply": "2021-02-09T12:09:50.983079Z"
        },
        "id": "N7brXNIGWXjC"
      },
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
        "\n",
        "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:50.988023Z",
          "iopub.status.busy": "2021-02-09T12:09:50.987348Z",
          "iopub.status.idle": "2021-02-09T12:09:50.995747Z",
          "shell.execute_reply": "2021-02-09T12:09:50.996175Z"
        },
        "id": "Xp-Y4mD6eDhF"
      },
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "\n",
        "An Agent represents algorithm used to solve an RL problem. TF-Agents provides standard implementations for the following Agents:\n",
        "\n",
        "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
        "-   [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
        "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
        "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
        "The DQN agent can be used in an environment which has a discrete action space.\n",
        "DQN Agent contains a `QNetwork`\n",
        ", a neural network model that can learn to predict QValues (expected returns) for all actions, given an observation from the environment.\n",
        "We will be using `tf_agents.networks``. to create a `QNetwork`. The network consists of a sequence of tf.keras.layers.Dense layers, where the final layer will have 1 output for each possible action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:09:51.003807Z",
          "iopub.status.busy": "2021-02-09T12:09:51.003118Z",
          "iopub.status.idle": "2021-02-09T12:09:51.031273Z",
          "shell.execute_reply": "2021-02-09T12:09:51.030645Z"
        },
        "id": "TgkdEPg_muzV"
      },
      "source": [
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu\n",
        "      #,\n",
        "      #kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "      #    scale=2.0, mode='fan_in', distribution='truncated_normal')\n",
        "      )\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# it's output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "\n",
        "DqnAgent is initialized using `tf_agents.agents.dqn.dqn_agent`. It takes `time_step_spec`, `action_spec`,`QNetwork` and an optimizer (`AdamOptimizer`) in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:00.416591Z",
          "iopub.status.busy": "2021-02-09T12:09:59.737207Z",
          "iopub.status.idle": "2021-02-09T12:10:01.180787Z",
          "shell.execute_reply": "2021-02-09T12:10:01.180174Z"
        },
        "id": "jbY4yrjTEyc9"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n",
        "\n",
        "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
        "\n",
        "In this tutorial:\n",
        "\n",
        "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
        "-   The policy returns an action (left or right) for each `time_step` observation.\n",
        "\n",
        "Agents contain two policies: \n",
        "\n",
        "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
        "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.185725Z",
          "iopub.status.busy": "2021-02-09T12:10:01.184911Z",
          "iopub.status.idle": "2021-02-09T12:10:01.187495Z",
          "shell.execute_reply": "2021-02-09T12:10:01.186939Z"
        },
        "id": "BwY7StuMkuV4"
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qs1Fl3dV0ae"
      },
      "source": [
        "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.192185Z",
          "iopub.status.busy": "2021-02-09T12:10:01.191377Z",
          "iopub.status.idle": "2021-02-09T12:10:01.193525Z",
          "shell.execute_reply": "2021-02-09T12:10:01.193969Z"
        },
        "id": "HE37-UCIrE69"
      },
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlnlRRsUbxP"
      },
      "source": [
        "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
        "\n",
        "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
        "-   `state` — used for stateful (that is, RNN-based) policies\n",
        "-   `info` — auxiliary data, such as log probabilities of actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.200308Z",
          "iopub.status.busy": "2021-02-09T12:10:01.199489Z",
          "iopub.status.idle": "2021-02-09T12:10:01.203315Z",
          "shell.execute_reply": "2021-02-09T12:10:01.202794Z"
        },
        "id": "5gCcpXswVAxk"
      },
      "source": [
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.207956Z",
          "iopub.status.busy": "2021-02-09T12:10:01.206874Z",
          "iopub.status.idle": "2021-02-09T12:10:01.209837Z",
          "shell.execute_reply": "2021-02-09T12:10:01.210247Z"
        },
        "id": "D4DHZtq3Ndis"
      },
      "source": [
        "time_step = example_environment.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.215620Z",
          "iopub.status.busy": "2021-02-09T12:10:01.214795Z",
          "iopub.status.idle": "2021-02-09T12:10:01.221386Z",
          "shell.execute_reply": "2021-02-09T12:10:01.220796Z"
        },
        "id": "PRFqAUzpNaAW"
      },
      "source": [
        "random_policy.action(time_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "Average return is used to evaluate policy. It is sum of rewards obtained by running a policy in an environment for an episode. Several episodes lead to an average return.\n",
        "Function below computes average return of policy for a given policy, environment and number of episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.227357Z",
          "iopub.status.busy": "2021-02-09T12:10:01.226532Z",
          "iopub.status.idle": "2021-02-09T12:10:01.228707Z",
          "shell.execute_reply": "2021-02-09T12:10:01.229105Z"
        },
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.233468Z",
          "iopub.status.busy": "2021-02-09T12:10:01.232808Z",
          "iopub.status.idle": "2021-02-09T12:10:01.747214Z",
          "shell.execute_reply": "2021-02-09T12:10:01.746678Z"
        },
        "id": "9bgU6Q6BZ8Bp"
      },
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "The replay buffer is used to keep track of data collected from the environment. This example uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`. \n",
        "The constructor of the class requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.752451Z",
          "iopub.status.busy": "2021-02-09T12:10:01.751798Z",
          "iopub.status.idle": "2021-02-09T12:10:01.763918Z",
          "shell.execute_reply": "2021-02-09T12:10:01.764374Z"
        },
        "id": "vX2zGUWJGWAl"
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.770403Z",
          "iopub.status.busy": "2021-02-09T12:10:01.769483Z",
          "iopub.status.idle": "2021-02-09T12:10:01.772650Z",
          "shell.execute_reply": "2021-02-09T12:10:01.773067Z"
        },
        "id": "_IZ-3HcqgE1z"
      },
      "source": [
        "agent.collect_data_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.777672Z",
          "iopub.status.busy": "2021-02-09T12:10:01.776781Z",
          "iopub.status.idle": "2021-02-09T12:10:01.781651Z",
          "shell.execute_reply": "2021-02-09T12:10:01.781123Z"
        },
        "id": "sy6g1tGcfRlw"
      },
      "source": [
        "agent.collect_data_spec._fields"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:01.787996Z",
          "iopub.status.busy": "2021-02-09T12:10:01.787329Z",
          "iopub.status.idle": "2021-02-09T12:10:02.216519Z",
          "shell.execute_reply": "2021-02-09T12:10:02.215857Z"
        },
        "id": "wr1KSAEGG4h9"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations. \n",
        "# For more details see the drivers module.\n",
        "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:02.220996Z",
          "iopub.status.busy": "2021-02-09T12:10:02.220245Z",
          "iopub.status.idle": "2021-02-09T12:10:02.222437Z",
          "shell.execute_reply": "2021-02-09T12:10:02.222878Z"
        },
        "id": "4wZnLu2ViO4E"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "iter(replay_buffer.as_dataset()).next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TujU-PMUsKjS"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
        "\n",
        "This dataset is also optimized by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:02.228262Z",
          "iopub.status.busy": "2021-02-09T12:10:02.227598Z",
          "iopub.status.idle": "2021-02-09T12:10:03.000822Z",
          "shell.execute_reply": "2021-02-09T12:10:03.000165Z"
        },
        "id": "ba7bilizt_qW"
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:03.005628Z",
          "iopub.status.busy": "2021-02-09T12:10:03.004273Z",
          "iopub.status.idle": "2021-02-09T12:10:03.066168Z",
          "shell.execute_reply": "2021-02-09T12:10:03.065640Z"
        },
        "id": "K13AST-2ppOq"
      },
      "source": [
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryzsaMsX0qeY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:03.070252Z",
          "iopub.status.busy": "2021-02-09T12:10:03.069431Z",
          "iopub.status.idle": "2021-02-09T12:10:03.072043Z",
          "shell.execute_reply": "2021-02-09T12:10:03.071523Z"
        },
        "id": "Th5w5Sff0b16"
      },
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data \n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "-   collect data from the environment\n",
        "-   use that data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodicially evaluates the policy and prints the current score.\n",
        "\n",
        "The following will take ~5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:10:03.082569Z",
          "iopub.status.busy": "2021-02-09T12:10:03.081857Z",
          "iopub.status.idle": "2021-02-09T12:16:31.001672Z",
          "shell.execute_reply": "2021-02-09T12:16:31.001077Z"
        },
        "id": "0pTbJ3PeyF-u"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "\n",
        "We will use `matplotlib.pyplot` to chart how the policy improved during training.\n",
        "\n",
        "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode possible is 200 and minimum is -200. The plot shows the return increasing towards that maximum each time it is evaluated during training. (It is a little unstable and not increase monotonically each time.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-09T12:16:31.022159Z",
          "iopub.status.busy": "2021-02-09T12:16:31.021544Z",
          "iopub.status.idle": "2021-02-09T12:16:31.143147Z",
          "shell.execute_reply": "2021-02-09T12:16:31.142668Z"
        },
        "id": "NxtL1mbOYCVO"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.title('Average Return vs Iterations for CartPole-v0: No Variance Scaling')\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}