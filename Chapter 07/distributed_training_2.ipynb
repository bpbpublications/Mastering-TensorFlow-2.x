{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Distribute\n",
    "\n",
    "Library for running a computation across multiple devices. `tf.distribute.Strategy` is a TensorFlow API to distribute training across multiple GPUs, multiple machines or TPUs. Using this API, developer can distribute existing models and training code with minimal code changes. Goal of this API is to\n",
    "\n",
    "* Target multiple personas - developers, researchers\n",
    "* Good performance out of the box\n",
    "* Easy of switching between various strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MirroredStrategy\n",
    "\n",
    "Synchronous training across multiple replicas on one machine. Implemented in class `tf.distribute.MirroredStrategy`. It extends class `tf.distribute.Strategy`.\n",
    "\n",
    "This strategy is typically used for training on one\n",
    "  machine with multiple GPUs. For TPUs, use\n",
    "  `tf.distribute.TPUStrategy`. To use `MirroredStrategy` with multiple workers,\n",
    "  please refer to `tf.distribute.experimental.MultiWorkerMirroredStrategy`.\n",
    "  For example, a variable created under a `MirroredStrategy` is a\n",
    "  `MirroredVariable`. If no devices are specified in the constructor argument of\n",
    "  the strategy then it will use all the available GPUs. If no GPUs are found, it\n",
    "  will use the available CPUs. Note that TensorFlow treats all CPUs on a\n",
    "  machine as a single device, and uses threads internally for parallelism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CollectiveCommunication.AUTO\n"
     ]
    }
   ],
   "source": [
    "multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiWorkerMirroredStrategy\n",
    "A distribution strategy for synchronous training on multiple workers. Inherits from `tf.distribute.Strategy`\n",
    "\n",
    "\n",
    "##### Collective Communication\n",
    "`tf.distribute.experimental.CollectiveCommunication`\n",
    "\n",
    "* AUTO\n",
    "* NCCL\n",
    "* RING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "WARNING:tensorflow:Enabled NCCL communication but no GPUs detected/specified.\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CollectiveCommunication.NCCL\n"
     ]
    }
   ],
   "source": [
    "multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n",
    "    tf.distribute.experimental.CollectiveCommunication.NCCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Central Storage Strategy\n",
    "\n",
    "We create a CentralStorageStrategy instance which will use all visible GPUs and CPU. Variables updated on all replicas are aggregates before applying to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\n"
     ]
    }
   ],
   "source": [
    "central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int64)\n",
      "<tensorflow.python.eager.def_function.Function object at 0x14dc43ac8>\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int64)\n",
      "<tensorflow.python.eager.def_function.Function object at 0x14dc43ac8>\n",
      "tf.Tensor([4], shape=(1,), dtype=int64)\n",
      "<tensorflow.python.eager.def_function.Function object at 0x14dc43ac8>\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.experimental.CentralStorageStrategy()\n",
    "# Create a dataset\n",
    "ds = tf.data.Dataset.range(5).batch(2)\n",
    "# Distribute that dataset\n",
    "dist_dataset = strategy.experimental_distribute_dataset(ds)\n",
    "\n",
    "with strategy.scope():\n",
    "  @tf.function\n",
    "  def train_step(val):\n",
    "    return val + 1\n",
    "\n",
    "  # Iterate over the distributed dataset\n",
    "  for x in dist_dataset:\n",
    "    # process dataset elements\n",
    "    print(x)\n",
    "    print(train_step)\n",
    "    strategy.run(train_step, args=(x,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
